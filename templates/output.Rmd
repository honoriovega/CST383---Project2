---
title: "Analysis of Video Game Sales Prediction"
author: "Honorio Vega, Andrew Sanchez, Bret Stine"
date: "May 3, 2018"
output: html_document
---
```{r global_options, include=TRUE}
knitr::opts_chunk$set(prompt=TRUE, comment="", echo=TRUE)
library(rpart)
library(rpart.plot)
library(maptree)
source("https://raw.githubusercontent.com/grbruns/cst383/master/lin-regr-util.R")
# perform n-fold cross-validation on the given data set; return mean rmse
# dat - a data frame
# y - response variable, as a string
# xs - predictor variables, as a vector of strings
# n   - the 'n' in n-fold cross-validation
cross_validate_lm = function(dat, y, xs, n=10) {
  # create the formula to be used with lm
  ff = reformulate(xs, y)
  
  # compute indexes of the groups
  k = nrow(dat)  
  dat1 = dat[sample(1:k),]     # shuffle the data
  starts = seq(1, k, by=floor(k/n))[1:n]
  ends = c(starts[2:n]-1, k)
  
  sum_rmse = 0
  for (i in 1:n) {
    tests = starts[i]:ends[i]
    fit = lm(ff, data=dat1[-tests,])
    if (length(fit$coefficients) > fit$rank) {
      print(paste0("rank-deficit problem with ", ff))
    }
    predicted = predict(fit, newdata=dat1[tests,])
    actual = dat1[tests,y]
    rmse = sqrt(mean((actual-predicted)^2))
    sum_rmse = sum_rmse + rmse
    # print(paste0(i,": RMSE = ",rmse))
  }
  return(sum_rmse/n)
}
```

### Goal/Hypothesis
Our team's goal is to predict global video game sales based off of data collected in the past 15 years. 

### Introduction
The data set is based off of video game sales as of January 2017. Origin of the data can be found using the link provided:  https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings. We decided to use this data set because of each member has an attachment to video games. Being able to predict future video game sales would be valuable to consumers and companies. 

### Read & Preprocessing
We read the data in and begin preprocessing. First thing we do is get the total number of NAs values and see that there is 37396 rows of missing data. In order to keep our data recent and relevent we decided to use complete cases. Since many of the games with a year of release lower than 2000s tended to have missing data such as global sales, critic score, and user score. We also scale user score from 0-100 that way it will resemble critics score scaling.

```{r }
setwd('..')
dat = read.csv(paste0(getwd(),"/data/Video_Game_Sales_as_of_Jan_2017.csv"))
names(dat) = tolower(names(dat))

# NAs
sum(is.na(dat))

# Before
before = nrow(dat)
before

# Remove any rows with missing data
dat = dat[complete.cases(dat),]

# throw away rows with year less than 2000
dat = dat[as.numeric(as.character(dat$year_of_release)) >= 2000,]
# Scaling user score to 0-100
dat$user_score = dat$user_score*10
```

The data set contains 17416 rows of data with a majority of games being released between 2000s and above.

```{r}
set.seed(123)

par(mar=c(3,7,1,1))
barplot(table(dat$year_of_release)[20:43], horiz=T, col="RED", las=1, main="Games by Year of Release")
```

After using complete cases we retain 40% of our data that is recent and relevant to our prediction of global sales. 

```{r}
dat = dat[complete.cases(dat),]
after = nrow(dat)
after/before

```

### Number of games in each Genre
Below is a barplot by games by Genre. We notice that Misc Genre had the most selling
games even though it accounts for less games. Based on the barplot, there are far more action
games. 
```{r}
par(mar=c(3,7,1,1))
barplot(sort(table(dat$genre)),horiz=TRUE,las=1,main="Number of games by genre",col="RED")
```


### Games by platform
```{r}
par(mar=c(3,7,1,1))
barplot(sort(table(dat$platform)),horiz=TRUE,las=1,main="Number of games by publisher",col="RED")
```

### Average Sales by Genres
We see that Misc is that top selling genre with mean sales of 1.06 globally. Genres like shooters and platform come in second with a mean sales of 0.92 globally.

```{r}
topGenres = aggregate(cbind(global_sales, na_sales, jp_sales, eu_sales) ~ genre, data=dat, function(x) mean(x))
topGenres = topGenres[order(-topGenres$global_sales),]
head(topGenres, 10)
```

### Highest Sales by Platform
We see in this data set that PS2 has highest sum of sales of all platforms. While X360 and PS3 come in second and third respectively. It is suprising to see PS2 have the highest sum of sales considering it was relased in March of 2000. 

```{r}
topPlatforms = aggregate(cbind(global_sales, na_sales, jp_sales, eu_sales) ~ platform, data=dat, function(x) sum(x))
topPlatforms = topPlatforms[order(-topPlatforms$global_sales),]
head(topPlatforms, 10)
```


### Critic Scores lead to Higher Global Sales?
From our plot we see that games that achieved a critic score greater than 50 tended to have higher global sales. While majority of games that didn't achieve sales of 1 Million are distributed between 20-100 on critic score.

```{r}
plot(global_sales[global_sales>1] ~ critic_score[global_sales>1], data=dat, col="RED", pch=20, xlab="Critic Score", ylab="Sales", main="Critic Score by Global Sales")
points(global_sales[global_sales<1] ~ critic_score[global_sales<1], data=dat, col="NAVY", pch=20)
```

### Cumlative Distribution of Critic Score
We see that the curve closely resembles one 

```{r}
plot(ecdf(dat$critic_score), col="RED", main="Critic Score")
lines(ecdf(dat$user_score), col="BLUE", main="User Score")
```

### Critic Scores vs User Scores by Genres
We see overall user scores are generally higher than critics scores. User scores are usually based off their experiences and impressions of a game, while critic scores are generally follow baselines of how to score a game.

```{r}
par(mfrow=c(1,2))
# Critic Score
plot(critic_score ~ genre, data=dat, main="Critic Score by Genre", xlab="Genre", ylab="Critic Score", col="RED")
# User Score
plot(user_score ~ genre, data=dat, main="User Score by Genre", xlab="Genre", ylab="User Score", col="RED")
```

### Differences Between Critics and Users
Here we get a closer view of critic and user scores and see for the most part there is small deviation. User scores tend to be higher per genre than their counter part critic scores.

```{r}
agg = aggregate(cbind(critic_score, user_score) ~ genre, data=dat, function(x) mean(x))
plot(agg$critic_score, agg$user_score, xlim=c(65,75), ylab="User Score", xlab="Critic Score", pch=20, main="Critics Score vs User Score")
text(agg$critic_score, agg$user_score, labels=agg$genre, pos=4, col="RED")
```

### Sampling & Splitting
We sample from the data and create training and test data sets with a 80/20 split respectively.
```{r}
# 
set.seed(123)

split = split_data(dat, c(0.8,0.2))
tr_dat = split[[1]]
te_dat = split[[2]]
```

### Cross Validation
Before we setup our first model we use cross validation to get an idea of what to expect with our first model. We achieved a RMSE score of 1.72 with features of critic score, user score, and genre.
```{r}
rmse_cv = cross_validate_lm(dat, "global_sales", c("critic_score", "user_score", "genre"))
rmse_cv
```

### Linear Regression (Model 1)
For our first model we decided to keep it simple and use a Linear regression model. We built our model with features of critic score, user score, and genre. From our summary we see that critic score and user score of ***, this in turn means they have strong relationship to global sales.

```{r}
fit = lm(global_sales ~ critic_score + user_score + genre, data=tr_dat)
summary(fit)
```

```{r}
# Plot
predicted = predict(fit, newdata=te_dat)
actual = te_dat$global_sales
plot_predict_actual(predicted, actual, 1.5, "Actual vs Predicted Global Sales")
```

We then test our linear regression model by computing the root mean squared error(RMSE). With RMSE the lower the value the better, with our model we achieved a RMSE value of 1.79.

```{r}
lmErrs1 = actual-predicted
rmse = sqrt(mean((actual-predicted)^2))
rmse
```

### Cross Validation
Here is the cross validated RMSE using the cross_validate_lm() function provided to us earlier in the semester. The mean RMSE is 1.6577. The goal for our models is to get an RMSE below the mean RMSE. Using cross_validate_lm() will give us a base line for our target RMSE.

```{r}
rmse_cv = cross_validate_lm(dat, "global_sales", c("platform", "genre", "user_count"))
rmse_cv
```

### Linear Regression (Model 2)
With our second model we decided to try and improve the our model by choosing different features. This time the features we choose are genre, platform, and user count. 

```{r}
fit = lm(global_sales ~ genre + platform + user_count, data=tr_dat)
summary(fit)
```

```{r}
# Plot
predicted = predict(fit, newdata=te_dat)
actual = te_dat$global_sales
plot_predict_actual(predicted, actual, 1.5, "Actual vs Predicted Global Sales")
```

This time with our second model we achieved lower RMSE score of 1.68, which is an improvement to the previous linear model.

```{r}
lmErrs2 = actual-predicted
rmse = sqrt(mean((actual-predicted)^2))
rmse
```

### Regression Tree (Model 3)
Having used Linear Regression and achieving an RMSE of 1.71, we decided to use a Regression Tree model to compare the two models and try to lower the RMSE of the Linear Regression. Unlike the previous model, Regression Tree uses the library rpart to create the model. Focusing on genre, platform, and user_count, we hope to achieve an RMSE of around 1.5 using a Regression Tree.

```{r}
fit = rpart(global_sales ~ genre + platform + user_count, data=tr_dat)
```

This is the plotted Regression Tree model using features genre, platform, and user_count. This Regression Tree split initially by user_score of 314. Based on how relevant each feature is, the tree split from there. Looking at the platform = PC, PSV, the split there is based off how popular a specific platform is and its relevance to global_sales. For the outcome of the Regression Tree, platform is the most relevant feature based on each ending platform leaf node.

```{r}
prp(fit, extra=1, varlen=-10,main="Regression Tree to Predict Global Sales", type = 2, tweak = 1.5, box.col="tan")
```

Here is the calculated RMSE using the testing data and the predicted fit. We achieved an RMSE of 1.495, which is lower than our predicted RMSE before we began the Regression Tree model. Comparing Regression Tree to Linear Regression,

```{r}
predicted = predict(fit, te_dat)
regErrs = te_dat$global_sales - predicted
rmse = sqrt(mean(regErrs^2))
rmse
```

Looking at the predicted line versus the actual points, the predicted line follows the points to a certain extent. However, the plot_predict_actual is designed to work with linear regression. Here, we are using it for Regression Trees, but the point of the plot is to show if the predicted line at all matches the points. From the plotted points, the predicted line follows the points and implies that the model works with the data.

```{r}
plot_predict_actual(predicted, te_dat$global_sales, 2, "Actual vs Predicted Global Sales")
```

Using the cross_validate_lm() function, the target RMSE was to get below 1.6752. The first model achieved an RMSE of 1.79, the second's RMSE is 1.682, and the third model achieved the lowest RMSE of 1.495. Our third model, using Regression Trees, is the best model to use when predicting global video game sales.

### Comparison of Models

```{r}
par(mfrow=c(1,3))
hist(lmErrs1, col="RED", main="Hist of Linear Regression", xlab="Errors")
hist(lmErrs2, col="RED", main="Hist of Linear Regression", xlab="Errors")
hist(regErrs, col="RED", main="Hist of Regression Tree Model", xlab="Errors")
```

### Our Conclusion
Based on our initial goal/hypothesis, we were able to predict global video game sales. Our Regression Tree model had the lowest RMSE out of the three models. Because of this, using Regression Tree to predict global video game sales is the best machine learning algorithm, based on the two algorithms we used. 