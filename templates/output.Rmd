---
title: "Using Regression to Predict Global Video Game Sales"
author: "Honorio Vega, Andrew Sanchez, Bret Stine"
date: "May 3, 2018"
output: html_document
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(prompt=TRUE, comment="", echo=TRUE)
library(rpart)
library(rpart.plot)
library(maptree)
source("https://raw.githubusercontent.com/grbruns/cst383/master/lin-regr-util.R")
# perform n-fold cross-validation on the given data set; return mean rmse
# dat - a data frame
# y - response variable, as a string
# xs - predictor variables, as a vector of strings
# n   - the 'n' in n-fold cross-validation
cross_validate_lm = function(dat, y, xs, n=10) {
  # create the formula to be used with lm
  ff = reformulate(xs, y)
  
  # compute indexes of the groups
  k = nrow(dat)  
  dat1 = dat[sample(1:k),]     # shuffle the data
  starts = seq(1, k, by=floor(k/n))[1:n]
  ends = c(starts[2:n]-1, k)
  
  sum_rmse = 0
  for (i in 1:n) {
    tests = starts[i]:ends[i]
    fit = lm(ff, data=dat1[-tests,])
    if (length(fit$coefficients) > fit$rank) {
      print(paste0("rank-deficit problem with ", ff))
    }
    predicted = predict(fit, newdata=dat1[tests,])
    actual = dat1[tests,y]
    rmse = sqrt(mean((actual-predicted)^2))
    sum_rmse = sum_rmse + rmse
    # print(paste0(i,": RMSE = ",rmse))
  }
  return(sum_rmse/n)
}
```

### Goal/Hypothesis
Our team's goal is to predict global video game sales based off of data collected in the past 15 years. 

### Introduction
The data set is based off of video game sales as of January 2017. Origin of the data can be found using the link provided:  https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings. We decided to use this data set because of each member has an attachment to video games. Being able to predict future video game sales would be valuable to consumers and companies. The dataset comes with a structure of factors, ints, and numeric values. With Platform, year of release, and genre being of factors types. While variables that deal with sales and scores are broken up into numeric/ints.

### Read & Preprocessing
We read the data in and begin preprocessing. First thing we do is get the total number of NAs values and see that there is 37396 rows of missing data. In order to keep our data recent and relevent we decided to use complete cases. Since many of the games with a year of release lower than 2000s tended to have missing data such as global sales, critic score, and user score. We also scale user score from 0-100 that way it will resemble critics score scaling.

```{r }
setwd('..')
dat = read.csv(paste0(getwd(),"/data/Video_Game_Sales_as_of_Jan_2017.csv"))
names(dat) = tolower(names(dat))

# NAs
sum(is.na(dat))

# Before
before = nrow(dat)
before

# Remove any rows with missing data
dat = dat[complete.cases(dat),]

# throw away rows with year less than 2000
dat = dat[as.numeric(as.character(dat$year_of_release)) >= 2000,]
# Scaling user score to 0-100
dat$user_score = dat$user_score*10
```

The data set contains 17416 rows of data with a majority of games being released between 2000s and above.

```{r}
set.seed(123)

par(mar=c(3,7,1,1))
barplot(table(dat$year_of_release)[20:43], horiz=T, col="RED", las=1, main="Games by Year of Release")
```

After using complete cases we retain 40% of our data that is recent and relevant to our prediction of global sales. 

```{r}
dat = dat[complete.cases(dat),]
after = nrow(dat)
after/before

```

### Number of games in each Genre
Below is barplot this breaks down games by their genre. From face value we see that action genres is types of games that are being produced. It's suprising to see that because as a group consensus, we typically played shooter genres.
```{r}
par(mar=c(3,7,1,1))
barplot(sort(table(dat$genre)),horiz=TRUE,las=1,main="Number of games by genre",col="RED")
```


### Games by Platform
In this plot we see that PS2 has the highest amount of games per platform with over 1000 games. Its suprising to see that XOne and PS4 have relatively low amount of games than Wii and DS platforms, since they are the current generation of consoles.
```{r}
par(mar=c(3,7,1,1))
barplot(sort(table(dat$platform)),horiz=TRUE,las=1,main="Number of Games by Platform",col="RED")
```

### Average Sales by Genres
We see that Misc is that top selling genre with mean sales of 1.06 globally. We notice that Misc Genre had the highest mean in sales compared to others. This was interesting because more Misc game small portion compared to other genres. Genres like shooters and platform come in second with a mean sales of 0.92 globally.

```{r}
topGenres = aggregate(cbind(global_sales, na_sales, jp_sales, eu_sales) ~ genre, data=dat, function(x) mean(x))
topGenres = topGenres[order(-topGenres$global_sales),]
head(topGenres, 10)
```

### Highest Sales by Platform
We see in this data set that PS2 has highest sum of sales of all platforms. While X360 and PS3 come in second and third respectively. It is suprising to see PS2 have the highest sum of sales considering it was relased in March of 2000. 

```{r}
topPlatforms = aggregate(cbind(global_sales, na_sales, jp_sales, eu_sales) ~ platform, data=dat, function(x) sum(x))
topPlatforms = topPlatforms[order(-topPlatforms$global_sales),]
head(topPlatforms, 10)
```


### Critic Scores lead to Higher Global Sales?
From our plot we see that games that achieved a critic score greater than 50 tended to have higher global sales. While majority of games that didn't achieve sales of 1 Million are distributed between 20-100 on critic score.

```{r}
plot(global_sales[global_sales>1] ~ critic_score[global_sales>1], data=dat, col="RED", pch=20, xlab="Critic Score", ylab="Sales", main="Critic Score by Global Sales")
points(global_sales[global_sales<1] ~ critic_score[global_sales<1], data=dat, col="NAVY", pch=20)
```

### Cumlative Distribution of Critic Score
In this plot we see critic score tended to curve earlier than user score, meaning there were more games that critics scored higher than users. The games scored by critics had more of even distribution while games scored by users tended to have an higher overall score in games.

```{r}
plot(ecdf(dat$critic_score), col="RED", main="Critic Score")
lines(ecdf(dat$user_score), col="BLUE", main="User Score")
```

### Critic Scores vs User Scores by Genres
We see overall user scores are generally higher than critics scores. User scores are usually based off their experiences and impressions of a game, while critic scores are generally follow baselines of how to score a game.

```{r}
par(mfrow=c(1,2))
# Critic Score
plot(critic_score ~ genre, data=dat, main="Critic Score by Genre", xlab="Genre", ylab="Critic Score", col="RED")
# User Score
plot(user_score ~ genre, data=dat, main="User Score by Genre", xlab="Genre", ylab="User Score", col="RED")
```

### Differences Between Critics and Users
Here we get a closer view of critic and user scores and see for the most part there is small deviation. User scores tend to be higher per genre than their counter part critic scores.

```{r}
agg = aggregate(cbind(critic_score, user_score) ~ genre, data=dat, function(x) mean(x))
plot(agg$critic_score, agg$user_score, xlim=c(65,75), ylab="User Score", xlab="Critic Score", pch=20, main="Critics Score vs User Score")
text(agg$critic_score, agg$user_score, labels=agg$genre, pos=4, col="RED")
```

### Sampling & Splitting
We sample from the data and create training and test data sets with a 80/20 split respectively.
```{r}
# 
set.seed(123)

split = split_data(dat, c(4,1))
tr_dat = split[[1]]
te_dat = split[[2]]
```

### Cross Validation
Before we setup our first model we use cross validation to get an idea of what to expect with our first model. We achieved a RMSE score of 1.72 with features of critic score, user score, and genre.
```{r}
rmse_cv = cross_validate_lm(dat, "global_sales", c("critic_score", "user_score", "genre"))
rmse_cv
```

### Linear Regression (Model 1)
For our first model we decided to keep it simple and use a Linear regression model. We built our model with features of critic score, user score, and genre. From our summary we see that critic score and user score of ***, this in turn means they have strong relationship to global sales.

```{r}
fit = lm(global_sales ~ critic_score + user_score + genre, data=tr_dat)
summary(fit)
```

```{r}
# Plot
predicted = predict(fit, newdata=te_dat)
actual = te_dat$global_sales
plot_predict_actual(predicted, actual, 1.5, "Actual vs Predicted Global Sales")
```

We then test our linear regression model by computing the root mean squared error(RMSE). With RMSE the lower the value the better, with our model we achieved a RMSE value of 1.79.

```{r}
lmErrs1 = actual-predicted
rmse = sqrt(mean((actual-predicted)^2))
rmse
```

After we get our see our rmse, we plot the fit to get a better understanding of our model. In our first plot Residuals vs Fitted what we're trying to achieve is level red line. Residuals is the difference between the data and our predicted line created in the linear regression model. As you can see we achieved a near flat line which in turn shows that our model assumptions hold true. The second plot we're trying achieve plotted points along the dotted red-line, this in turn is testing gaussian.  
```{r}
plot(fit)
```



### Cross Validation
Here is the cross validated RMSE using the cross_validate_lm() function provided to us earlier in the semester. The mean RMSE is 1.6577. The goal for our models is to get an RMSE below the mean RMSE. Using cross_validate_lm() will give us a base line for our target RMSE.

```{r}
rmse_cv = cross_validate_lm(dat, "global_sales", c("platform", "genre", "user_count"))
rmse_cv
```

### Linear Regression (Model 2)
With our second model we decided to try and improve the our model by choosing different features. This time the features we choose are genre, platform, and user count. 

```{r}
fit = lm(global_sales ~ genre + platform + user_count, data=tr_dat)
summary(fit)
```

```{r}
# Plot
predicted = predict(fit, newdata=te_dat)
actual = te_dat$global_sales
plot_predict_actual(predicted, actual, 1.5, "Actual vs Predicted Global Sales")
```

This time with our second model we achieved lower RMSE score of 1.68, which is an improvement from the previous linear model.

```{r}
lmErrs2 = actual-predicted
rmse = sqrt(mean((actual-predicted)^2))
rmse
```


After the RMSE plot, we run our diagnostic plots to better interpret out model. We see in the first plot the our red line nearly resembles a straight line, which in turn means our models assumptions hold true. As for the second plot, it didn't do as well as the first linear regression model as what we're looking for is points plotted along the dotted line. In the beginning and end the points tend to fall stray off.
```{r}
plot(fit)
```


### Regression Tree (Model 3)
Having used Linear Regression and achieving an RMSE of 1.71, we decided to use a Regression Tree model to compare the two models and try to lower the RMSE of the Linear Regression. Unlike the previous model, Regression Tree uses the library rpart to create the model. Focusing on genre, platform, and user_count, we hope to achieve an RMSE of around 1.5 using a Regression Tree.

```{r}
fit = rpart(global_sales ~ genre + platform + user_count, data=tr_dat)
```

This is the plotted Regression Tree model using features genre, platform, and user_count. This Regression Tree split initially by user_score of 314. Based on how relevant each feature is, the tree split from there. Looking at the platform = PC, PSV, the split there is based off how popular a specific platform is and its relevance to global_sales. For the outcome of the Regression Tree, platform is the most relevant feature based on each ending platform leaf node.

```{r}
prp(fit, extra=1, varlen=-10,main="Regression Tree to Predict Global Sales", type = 2, tweak = 1.5, box.col="tan")
```

Here is the calculated RMSE using the testing data and the predicted fit. We achieved an RMSE of 1.495, which is lower than our predicted RMSE before we began the Regression Tree model. Comparing Regression Tree to Linear Regression,

```{r}
predicted = predict(fit, te_dat)
regErrs = te_dat$global_sales - predicted
rmse = sqrt(mean(regErrs^2))
rmse
```

Looking at the predicted line versus the actual points, the predicted line follows the points to a certain extent. However, the plot_predict_actual is designed to work with linear regression. Here, we are using it for Regression Trees, but the point of the plot is to show if the predicted line at all matches the points. From the plotted points, the predicted line follows the points and implies that the model works with the data.

```{r}
plot_predict_actual(predicted, te_dat$global_sales, 1.5, "Actual vs Predicted Global Sales")
```

Using the cross_validate_lm() function, the target RMSE was to get below 1.6752. The first model achieved an RMSE of 1.79, the second's RMSE is 1.682, and the third model achieved the lowest RMSE of 1.495. Our third model, using Regression Trees, is the best model to use when predicting global video game sales.

### Comparison of Models
```{r}
par(mfrow=c(1,3))
hist(lmErrs1, col="RED", main="Hist of Linear Regression", xlab="Errors")
hist(lmErrs2, col="RED", main="Hist of Linear Regression", xlab="Errors")
hist(regErrs, col="RED", main="Hist of Regression Tree Model", xlab="Errors")
```

### Our Conclusion
Based on our initial goal/hypothesis, we were able to predict global video game sales. Our Regression Tree model achieved the lowest RMSE out of the three models. Because of this, using Regression Tree to predict global video game sales was the best machine learning algorithm, based on the two algorithms we used. There was also some interesting things in the data, one such misc had the highest mean of sales compared to other genres; Misc is a much smaller percentage of games compared to other genres such as actions and sports games. Games that achieved high critic score tended to have higher global sales compared to games that did not achieve a good critic score.

### Libraries and Functions used in the project
```{r}
library(rpart)
library(rpart.plot)
library(maptree)
source("https://raw.githubusercontent.com/grbruns/cst383/master/lin-regr-util.R")
# perform n-fold cross-validation on the given data set; return mean rmse
# dat - a data frame
# y - response variable, as a string
# xs - predictor variables, as a vector of strings
# n   - the 'n' in n-fold cross-validation
cross_validate_lm = function(dat, y, xs, n=10) {
  # create the formula to be used with lm
  ff = reformulate(xs, y)
  
  # compute indexes of the groups
  k = nrow(dat)  
  dat1 = dat[sample(1:k),]     # shuffle the data
  starts = seq(1, k, by=floor(k/n))[1:n]
  ends = c(starts[2:n]-1, k)
  
  sum_rmse = 0
  for (i in 1:n) {
    tests = starts[i]:ends[i]
    fit = lm(ff, data=dat1[-tests,])
    if (length(fit$coefficients) > fit$rank) {
      print(paste0("rank-deficit problem with ", ff))
    }
    predicted = predict(fit, newdata=dat1[tests,])
    actual = dat1[tests,y]
    rmse = sqrt(mean((actual-predicted)^2))
    sum_rmse = sum_rmse + rmse
    # print(paste0(i,": RMSE = ",rmse))
  }
  return(sum_rmse/n)
}
```